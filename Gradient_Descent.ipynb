{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Gradient Descent.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MyDearGreatTeacher/AI4high/blob/master/Gradient_Descent.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "r1k32PRP7OoC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Gradient Descent\n",
        "\n",
        "\n",
        "CHAPTER 8:Gradient Descent\n",
        "\n",
        "https://github.com/joelgrus/data-science-from-scratch/tree/master/code-python3"
      ]
    },
    {
      "metadata": {
        "id": "ancaW1ir6oTo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "outputId": "2d75c0d2-55eb-48c2-cb0a-86748e40d7e0"
      },
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "from linear_algebra import distance, vector_subtract, scalar_multiply\n",
        "from functools import reduce\n",
        "import math, random\n",
        "\n",
        "def sum_of_squares(v):\n",
        "    \"\"\"computes the sum of squared elements in v\"\"\"\n",
        "    return sum(v_i ** 2 for v_i in v)\n",
        "\n",
        "def difference_quotient(f, x, h):\n",
        "    return (f(x + h) - f(x)) / h\n",
        "\n",
        "def plot_estimated_derivative():\n",
        "\n",
        "    def square(x):\n",
        "        return x * x\n",
        "\n",
        "    def derivative(x):\n",
        "        return 2 * x\n",
        "\n",
        "    derivative_estimate = lambda x: difference_quotient(square, x, h=0.00001)\n",
        "\n",
        "    # plot to show they're basically the same\n",
        "    import matplotlib.pyplot as plt\n",
        "    x = range(-10,10)\n",
        "    plt.plot(x, map(derivative, x), 'rx')           # red  x\n",
        "    plt.plot(x, map(derivative_estimate, x), 'b+')  # blue +\n",
        "    plt.show()                                      # purple *, hopefully\n",
        "\n",
        "def partial_difference_quotient(f, v, i, h):\n",
        "\n",
        "    # add h to just the i-th element of v\n",
        "    w = [v_j + (h if j == i else 0)\n",
        "         for j, v_j in enumerate(v)]\n",
        "\n",
        "    return (f(w) - f(v)) / h\n",
        "\n",
        "def estimate_gradient(f, v, h=0.00001):\n",
        "    return [partial_difference_quotient(f, v, i, h)\n",
        "            for i, _ in enumerate(v)]\n",
        "\n",
        "def step(v, direction, step_size):\n",
        "    \"\"\"move step_size in the direction from v\"\"\"\n",
        "    return [v_i + step_size * direction_i\n",
        "            for v_i, direction_i in zip(v, direction)]\n",
        "\n",
        "def sum_of_squares_gradient(v):\n",
        "    return [2 * v_i for v_i in v]\n",
        "\n",
        "def safe(f):\n",
        "    \"\"\"define a new function that wraps f and return it\"\"\"\n",
        "    def safe_f(*args, **kwargs):\n",
        "        try:\n",
        "            return f(*args, **kwargs)\n",
        "        except:\n",
        "            return float('inf')         # this means \"infinity\" in Python\n",
        "    return safe_f\n",
        "\n",
        "\n",
        "#\n",
        "#\n",
        "# minimize / maximize batch\n",
        "#\n",
        "#\n",
        "\n",
        "def minimize_batch(target_fn, gradient_fn, theta_0, tolerance=0.000001):\n",
        "    \"\"\"use gradient descent to find theta that minimizes target function\"\"\"\n",
        "\n",
        "    step_sizes = [100, 10, 1, 0.1, 0.01, 0.001, 0.0001, 0.00001]\n",
        "\n",
        "    theta = theta_0                           # set theta to initial value\n",
        "    target_fn = safe(target_fn)               # safe version of target_fn\n",
        "    value = target_fn(theta)                  # value we're minimizing\n",
        "\n",
        "    while True:\n",
        "        gradient = gradient_fn(theta)\n",
        "        next_thetas = [step(theta, gradient, -step_size)\n",
        "                       for step_size in step_sizes]\n",
        "\n",
        "        # choose the one that minimizes the error function\n",
        "        next_theta = min(next_thetas, key=target_fn)\n",
        "        next_value = target_fn(next_theta)\n",
        "\n",
        "        # stop if we're \"converging\"\n",
        "        if abs(value - next_value) < tolerance:\n",
        "            return theta\n",
        "        else:\n",
        "            theta, value = next_theta, next_value\n",
        "\n",
        "def negate(f):\n",
        "    \"\"\"return a function that for any input x returns -f(x)\"\"\"\n",
        "    return lambda *args, **kwargs: -f(*args, **kwargs)\n",
        "\n",
        "def negate_all(f):\n",
        "    \"\"\"the same when f returns a list of numbers\"\"\"\n",
        "    return lambda *args, **kwargs: [-y for y in f(*args, **kwargs)]\n",
        "\n",
        "def maximize_batch(target_fn, gradient_fn, theta_0, tolerance=0.000001):\n",
        "    return minimize_batch(negate(target_fn),\n",
        "                          negate_all(gradient_fn),\n",
        "                          theta_0,\n",
        "                          tolerance)\n",
        "\n",
        "#\n",
        "# minimize / maximize stochastic\n",
        "#\n",
        "\n",
        "def in_random_order(data):\n",
        "    \"\"\"generator that returns the elements of data in random order\"\"\"\n",
        "    indexes = [i for i, _ in enumerate(data)]  # create a list of indexes\n",
        "    random.shuffle(indexes)                    # shuffle them\n",
        "    for i in indexes:                          # return the data in that order\n",
        "        yield data[i]\n",
        "\n",
        "def minimize_stochastic(target_fn, gradient_fn, x, y, theta_0, alpha_0=0.01):\n",
        "\n",
        "    data = list(zip(x, y))\n",
        "    theta = theta_0                             # initial guess\n",
        "    alpha = alpha_0                             # initial step size\n",
        "    min_theta, min_value = None, float(\"inf\")   # the minimum so far\n",
        "    iterations_with_no_improvement = 0\n",
        "\n",
        "    # if we ever go 100 iterations with no improvement, stop\n",
        "    while iterations_with_no_improvement < 100:\n",
        "        value = sum( target_fn(x_i, y_i, theta) for x_i, y_i in data )\n",
        "\n",
        "        if value < min_value:\n",
        "            # if we've found a new minimum, remember it\n",
        "            # and go back to the original step size\n",
        "            min_theta, min_value = theta, value\n",
        "            iterations_with_no_improvement = 0\n",
        "            alpha = alpha_0\n",
        "        else:\n",
        "            # otherwise we're not improving, so try shrinking the step size\n",
        "            iterations_with_no_improvement += 1\n",
        "            alpha *= 0.9\n",
        "\n",
        "        # and take a gradient step for each of the data points\n",
        "        for x_i, y_i in in_random_order(data):\n",
        "            gradient_i = gradient_fn(x_i, y_i, theta)\n",
        "            theta = vector_subtract(theta, scalar_multiply(alpha, gradient_i))\n",
        "\n",
        "    return min_theta\n",
        "\n",
        "def maximize_stochastic(target_fn, gradient_fn, x, y, theta_0, alpha_0=0.01):\n",
        "    return minimize_stochastic(negate(target_fn),\n",
        "                               negate_all(gradient_fn),\n",
        "                               x, y, theta_0, alpha_0)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    print(\"using the gradient\")\n",
        "\n",
        "    v = [random.randint(-10,10) for i in range(3)]\n",
        "\n",
        "    tolerance = 0.0000001\n",
        "\n",
        "    while True:\n",
        "        #print v, sum_of_squares(v)\n",
        "        gradient = sum_of_squares_gradient(v)   # compute the gradient at v\n",
        "        next_v = step(v, gradient, -0.01)       # take a negative gradient step\n",
        "        if distance(next_v, v) < tolerance:     # stop if we're converging\n",
        "            break\n",
        "        v = next_v                              # continue if we're not\n",
        "\n",
        "    print(\"minimum v\", v)\n",
        "    print(\"minimum value\", sum_of_squares(v))\n",
        "    print()\n",
        "\n",
        "\n",
        "    print(\"using minimize_batch\")\n",
        "\n",
        "    v = [random.randint(-10,10) for i in range(3)]\n",
        "\n",
        "    v = minimize_batch(sum_of_squares, sum_of_squares_gradient, v)\n",
        "\n",
        "    print(\"minimum v\", v)\n",
        "    print(\"minimum value\", sum_of_squares(v))"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-6dfe3db7fb24>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcollections\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCounter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mlinear_algebra\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdistance\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvector_subtract\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscalar_multiply\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mfunctools\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'linear_algebra'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "-SocVqfA65Z_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 134
        },
        "outputId": "2454c029-afa0-4829-807b-5e0dbe48176a"
      },
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "#from linear_algebra import distance, vector_subtract, scalar_multiply\n",
        "from functools import reduce\n",
        "import math, random\n",
        "\n",
        "\n",
        "def vector_add(v, w):\n",
        "    \"\"\"adds two vectors componentwise\"\"\"\n",
        "    return [v_i + w_i for v_i, w_i in zip(v,w)]\n",
        "\n",
        "def vector_subtract(v, w):\n",
        "    \"\"\"subtracts two vectors componentwise\"\"\"\n",
        "    return [v_i - w_i for v_i, w_i in zip(v,w)]\n",
        "\n",
        "def vector_sum(vectors):\n",
        "    return reduce(vector_add, vectors)\n",
        "\n",
        "def scalar_multiply(c, v):\n",
        "    return [c * v_i for v_i in v]\n",
        "\n",
        "def vector_mean(vectors):\n",
        "    \"\"\"compute the vector whose i-th element is the mean of the\n",
        "    i-th elements of the input vectors\"\"\"\n",
        "    n = len(vectors)\n",
        "    return scalar_multiply(1/n, vector_sum(vectors))\n",
        "\n",
        "def dot(v, w):\n",
        "    \"\"\"v_1 * w_1 + ... + v_n * w_n\"\"\"\n",
        "    return sum(v_i * w_i for v_i, w_i in zip(v, w))\n",
        "\n",
        "def sum_of_squares(v):\n",
        "    \"\"\"v_1 * v_1 + ... + v_n * v_n\"\"\"\n",
        "    return dot(v, v)\n",
        "\n",
        "def magnitude(v):\n",
        "    return math.sqrt(sum_of_squares(v))\n",
        "\n",
        "def squared_distance(v, w):\n",
        "    return sum_of_squares(vector_subtract(v, w))\n",
        "\n",
        "def distance(v, w):\n",
        "   return math.sqrt(squared_distance(v, w))\n",
        "\n",
        "def sum_of_squares(v):\n",
        "    \"\"\"computes the sum of squared elements in v\"\"\"\n",
        "    return sum(v_i ** 2 for v_i in v)\n",
        "\n",
        "def difference_quotient(f, x, h):\n",
        "    return (f(x + h) - f(x)) / h\n",
        "\n",
        "def plot_estimated_derivative():\n",
        "\n",
        "    def square(x):\n",
        "        return x * x\n",
        "\n",
        "    def derivative(x):\n",
        "        return 2 * x\n",
        "\n",
        "    derivative_estimate = lambda x: difference_quotient(square, x, h=0.00001)\n",
        "\n",
        "    # plot to show they're basically the same\n",
        "    import matplotlib.pyplot as plt\n",
        "    x = range(-10,10)\n",
        "    plt.plot(x, map(derivative, x), 'rx')           # red  x\n",
        "    plt.plot(x, map(derivative_estimate, x), 'b+')  # blue +\n",
        "    plt.show()                                      # purple *, hopefully\n",
        "\n",
        "def partial_difference_quotient(f, v, i, h):\n",
        "\n",
        "    # add h to just the i-th element of v\n",
        "    w = [v_j + (h if j == i else 0)\n",
        "         for j, v_j in enumerate(v)]\n",
        "\n",
        "    return (f(w) - f(v)) / h\n",
        "\n",
        "def estimate_gradient(f, v, h=0.00001):\n",
        "    return [partial_difference_quotient(f, v, i, h)\n",
        "            for i, _ in enumerate(v)]\n",
        "\n",
        "def step(v, direction, step_size):\n",
        "    \"\"\"move step_size in the direction from v\"\"\"\n",
        "    return [v_i + step_size * direction_i\n",
        "            for v_i, direction_i in zip(v, direction)]\n",
        "\n",
        "def sum_of_squares_gradient(v):\n",
        "    return [2 * v_i for v_i in v]\n",
        "\n",
        "def safe(f):\n",
        "    \"\"\"define a new function that wraps f and return it\"\"\"\n",
        "    def safe_f(*args, **kwargs):\n",
        "        try:\n",
        "            return f(*args, **kwargs)\n",
        "        except:\n",
        "            return float('inf')         # this means \"infinity\" in Python\n",
        "    return safe_f\n",
        "\n",
        "\n",
        "#\n",
        "#\n",
        "# minimize / maximize batch\n",
        "#\n",
        "#\n",
        "\n",
        "def minimize_batch(target_fn, gradient_fn, theta_0, tolerance=0.000001):\n",
        "    \"\"\"use gradient descent to find theta that minimizes target function\"\"\"\n",
        "\n",
        "    step_sizes = [100, 10, 1, 0.1, 0.01, 0.001, 0.0001, 0.00001]\n",
        "\n",
        "    theta = theta_0                           # set theta to initial value\n",
        "    target_fn = safe(target_fn)               # safe version of target_fn\n",
        "    value = target_fn(theta)                  # value we're minimizing\n",
        "\n",
        "    while True:\n",
        "        gradient = gradient_fn(theta)\n",
        "        next_thetas = [step(theta, gradient, -step_size)\n",
        "                       for step_size in step_sizes]\n",
        "\n",
        "        # choose the one that minimizes the error function\n",
        "        next_theta = min(next_thetas, key=target_fn)\n",
        "        next_value = target_fn(next_theta)\n",
        "\n",
        "        # stop if we're \"converging\"\n",
        "        if abs(value - next_value) < tolerance:\n",
        "            return theta\n",
        "        else:\n",
        "            theta, value = next_theta, next_value\n",
        "\n",
        "def negate(f):\n",
        "    \"\"\"return a function that for any input x returns -f(x)\"\"\"\n",
        "    return lambda *args, **kwargs: -f(*args, **kwargs)\n",
        "\n",
        "def negate_all(f):\n",
        "    \"\"\"the same when f returns a list of numbers\"\"\"\n",
        "    return lambda *args, **kwargs: [-y for y in f(*args, **kwargs)]\n",
        "\n",
        "def maximize_batch(target_fn, gradient_fn, theta_0, tolerance=0.000001):\n",
        "    return minimize_batch(negate(target_fn),\n",
        "                          negate_all(gradient_fn),\n",
        "                          theta_0,\n",
        "                          tolerance)\n",
        "\n",
        "#\n",
        "# minimize / maximize stochastic\n",
        "#\n",
        "\n",
        "def in_random_order(data):\n",
        "    \"\"\"generator that returns the elements of data in random order\"\"\"\n",
        "    indexes = [i for i, _ in enumerate(data)]  # create a list of indexes\n",
        "    random.shuffle(indexes)                    # shuffle them\n",
        "    for i in indexes:                          # return the data in that order\n",
        "        yield data[i]\n",
        "\n",
        "def minimize_stochastic(target_fn, gradient_fn, x, y, theta_0, alpha_0=0.01):\n",
        "\n",
        "    data = list(zip(x, y))\n",
        "    theta = theta_0                             # initial guess\n",
        "    alpha = alpha_0                             # initial step size\n",
        "    min_theta, min_value = None, float(\"inf\")   # the minimum so far\n",
        "    iterations_with_no_improvement = 0\n",
        "\n",
        "    # if we ever go 100 iterations with no improvement, stop\n",
        "    while iterations_with_no_improvement < 100:\n",
        "        value = sum( target_fn(x_i, y_i, theta) for x_i, y_i in data )\n",
        "\n",
        "        if value < min_value:\n",
        "            # if we've found a new minimum, remember it\n",
        "            # and go back to the original step size\n",
        "            min_theta, min_value = theta, value\n",
        "            iterations_with_no_improvement = 0\n",
        "            alpha = alpha_0\n",
        "        else:\n",
        "            # otherwise we're not improving, so try shrinking the step size\n",
        "            iterations_with_no_improvement += 1\n",
        "            alpha *= 0.9\n",
        "\n",
        "        # and take a gradient step for each of the data points\n",
        "        for x_i, y_i in in_random_order(data):\n",
        "            gradient_i = gradient_fn(x_i, y_i, theta)\n",
        "            theta = vector_subtract(theta, scalar_multiply(alpha, gradient_i))\n",
        "\n",
        "    return min_theta\n",
        "\n",
        "def maximize_stochastic(target_fn, gradient_fn, x, y, theta_0, alpha_0=0.01):\n",
        "    return minimize_stochastic(negate(target_fn),\n",
        "                               negate_all(gradient_fn),\n",
        "                               x, y, theta_0, alpha_0)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    print(\"using the gradient\")\n",
        "\n",
        "    v = [random.randint(-10,10) for i in range(3)]\n",
        "\n",
        "    tolerance = 0.0000001\n",
        "\n",
        "    while True:\n",
        "        #print v, sum_of_squares(v)\n",
        "        gradient = sum_of_squares_gradient(v)   # compute the gradient at v\n",
        "        next_v = step(v, gradient, -0.01)       # take a negative gradient step\n",
        "        if distance(next_v, v) < tolerance:     # stop if we're converging\n",
        "            break\n",
        "        v = next_v                              # continue if we're not\n",
        "\n",
        "    print(\"minimum v\", v)\n",
        "    print(\"minimum value\", sum_of_squares(v))\n",
        "    print()\n",
        "\n",
        "\n",
        "    print(\"using minimize_batch\")\n",
        "\n",
        "    v = [random.randint(-10,10) for i in range(3)]\n",
        "\n",
        "    v = minimize_batch(sum_of_squares, sum_of_squares_gradient, v)\n",
        "\n",
        "    print(\"minimum v\", v)\n",
        "    print(\"minimum value\", sum_of_squares(v))"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "using the gradient\n",
            "minimum v [-5.222400758641384e-07, -1.5667202275924157e-06, 4.700160682777249e-06]\n",
            "minimum value 2.481885741231091e-11\n",
            "\n",
            "using minimize_batch\n",
            "minimum v [0.001038459371706966, 0.0012980742146337075, 0.0]\n",
            "minimum value 2.763394533382943e-06\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}